+++
# Project title.
title = "Mini SQL Engine"

# Date this page was created.
date = 2018-01-15T00:00:00

# Project summary to display on homepage.
summary = "A Mini-SQL engine which will run a subset of SQL queries using command line interface."

# Tags: can be used for filtering projects.
# Example: `tags = ["machine-learning", "deep-learning"]`
tags = ["Database","Python", "MySQL"]

# Optional external URL for project (replaces project detail page).
external_link = ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""                                                                                                                                                                                                                                                                                                                                                   

# Links (optional).
url_pdf = "https://github.com/iamar7/mini-sql-engine/blob/master/CSE441_%20DATABASE%20SYSTEMS%20ASSIGNMENT%201.pdf"
url_slides = ""
url_video = ""
url_code = "https://github.com/iamar7/mini-sql-engine"

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
# url_custom = [{icon_pack = "fab", icon="twitter", name="Follow", url = "https://twitter.com/georgecushen"}]

# Featured image
# To use, add an image named `featured.jpg/png` to your project's folder.
[image]
  # Caption (optional)
  caption = "Image credit: [**The Virtual Brain**](https://www.thevirtualbrain.org/tvb/zwei)"

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Smart"
+++

<!-- An autoencoder(AE) is a type of artificial neural network used to learn efficient data encodings in an unsupervised manner. A variational autoencoder(VAE) is a specific type of autoencoder that helps to 'learn' complex probabilistic models based on the input data set. Using the learned representative models (also called latent space), we can generate 'novel' and 'unseen' variations of the learned latent space. In this project, we try to build variational autoencoders for image generation on MNIST,CIFAR10 and CALTECH101 dataset. -->
